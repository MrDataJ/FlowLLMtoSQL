{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlowGPT White Paper\n",
    "This [project](www.mrdataj.com/chatbot) is an innovative AI-driven Django application that leverages OpenAI's GPT-3.5 model to translate structured text into actionable insights. The application transforms free-text queries into SQL code using a series of chained calls, demonstrating an impressive integration of natural language understanding and code generation. This SQL code is then executed via the Flipside API, a powerful data management platform that runs the queries and returns a resulting table.\n",
    "\n",
    "The application further capitalizes on the abilities of GPT-3.5 to analyze the results and generate intelligent commentary. These insights are subsequently displayed on the Django frontend, providing users with a clear and comprehensive understanding of the information derived from their original free-text input. This application thus significantly lowers the barrier to data analysis, potentially, in the near future making complex data queries as simple as typing a question in natural language."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "Standard stuff.\n",
    "- Literature Review\n",
    "- Examine Existing Frameworks\n",
    "- Punch Tape\n",
    "- Incorporate Learning Methods\n",
    "- Testing and Validation\n",
    "- Beers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "# Requirements.txt\n",
    "% pip install openai\n",
    "% pip install flipside\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import openai\n",
    "import os\n",
    "from flipside import Flipside\n",
    "from dotenv import load_dotenv\n",
    "import settings\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPENAI\n",
    "My journey began with connecting to OpenAI API and seeing if i can generate a response from a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API Key\n",
    "openai.api_key = os.getenv(\"OPENAI_API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I believe the sky is actually blue. Can you elaborate on what you meant by \"the sky green\"?\n"
     ]
    }
   ],
   "source": [
    "# My Hello World OpenAI Function\n",
    "def ask_openai(message):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model = \"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an GPT agent, and I am a human. Ask me anything.\"},\n",
    "            {\"role\": \"user\", \"content\": message},\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content.strip()\n",
    "    return answer\n",
    "\n",
    "response = ask_openai(\"The sky green.\")\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You win this round AI...\n",
    "## Which Model?\n",
    "Many things can be taken into consideration when deciding which model to use from OpenAIs offering. There are various levels of Instruct models than even allow fine tuning. Given there is a large resource of questions with working SQL code answers in the public dashboards on flipside, training these models wouldnt have been a bad experiment if not time consuming one. Chat models on the otherhand promise better conversational replies and more human interactions. \n",
    "\n",
    "But ultimately I landed on GPT-3.5-turbo model as its CHEAPER."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations and Work-arounds\n",
    "- Dated Knowledge: ChatGPT doesnt know the databases of Flipside and specifically the Flow tables schema. This will have to be taught.\n",
    "- Token Count: OpenAI API has a limitation to how much data you can give and recieve in one call. \n",
    "- Finances: Given i am not funded by CZ (yet at least) i wanted to limit the number of OpenAI API calls i make and the size of these calls.\n",
    "\n",
    "Bring in model prompts, templates and chaining.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prompt and Templates\n",
    "If you've used ChatGPT you'll know the benefit of first giving ChatGPT a role or persona before asking it questions. Same applies here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7O0zSkfWvXYEaDKqUBuTAs0qvDxcr at 0x2418079f5f0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"Assuming that there is a table named `transactions` with a column `transaction_date` that stores the transaction date in the format 'YYYY-MM-DD', the SQL code to count the number of transactions on July 21, 2022 would be:\\n\\n```\\nSELECT COUNT(*) FROM transactions WHERE transaction_date = '2022-07-21';\\n```\\n\\nThis code will return the count of all transactions that occurred on July 21, 2022.\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1685958034,\n",
       "  \"id\": \"chatcmpl-7O0zSkfWvXYEaDKqUBuTAs0qvDxcr\",\n",
       "  \"model\": \"gpt-3.5-turbo-0301\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 92,\n",
       "    \"prompt_tokens\": 91,\n",
       "    \"total_tokens\": 183\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test user input message\n",
    "user_prompt = 'How many transactions where on july 21st 2022?'\n",
    "\n",
    "# Give GPT the role of a SQL master\n",
    "system_prompt = 'You are an SQL master. Only you can answer the following questions with strict SQL code, nothing else.'\n",
    "\n",
    "# Attempt to train it on a sample database schema\n",
    "db_prompt = open('db_prompt.txt', 'r').read()\n",
    "\n",
    "# Reiterate snowflake SQL - flipsides db, attach user question to the end\n",
    "prompt_template = '''\n",
    "Answer the following questions by writing suggested strict snowSQL code, nothing else. {user_prompt}\n",
    "'''\n",
    "\n",
    "openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": db_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": \"I now have the db schema, i can write sql code to create a table\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_template.format(user_prompt=user_prompt)},\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "- Despite asking a couple of times, it never responds in pure SQL code therefore will have to parse the response to pull out the code.\n",
    "- Inputting just the schema is doesnt know what the information in the tables is so has made some assumptions. Accuracy would improve if i gave it some sample rows, the literature has shown this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT to SQL\n",
    "Quick function to parse the output to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSELECT COUNT(*) \\nFROM FACT_TRANSACTIONS\\nWHERE CONVERT_TIMEZONE('UTC','America/New_York', BLOCK_TIMESTAMP::TIMESTAMP) BETWEEN '2022-07-21 00:00:00' AND '2022-07-21 23:59:59';\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Funtion for converting GPT output to SQL\n",
    "import re\n",
    "\n",
    "test_response = {\"content\": \"Assuming that the `BLOCK_TIMESTAMP` column stores the timestamp for each transaction, the following SnowSQL query can be used to count the number of transactions that occurred on July 21st, 2022:\\n\\n```\\nSELECT COUNT(*) \\nFROM FACT_TRANSACTIONS\\nWHERE CONVERT_TIMEZONE('UTC','America/New_York', BLOCK_TIMESTAMP::TIMESTAMP) BETWEEN '2022-07-21 00:00:00' AND '2022-07-21 23:59:59';\\n```\\n\\nThis query converts the time zone of the `BLOCK_TIMESTAMP` values from UTC to America/New_York to match the date range of interest (July 21st, 2022). It then selects all transactions that occurred during that day and counts them using the `COUNT` function.\",}\n",
    "\n",
    "def gpt2sql(gptstring):\n",
    "       pattern = r\"```(.*)```\"\n",
    "       match = re.findall(pattern=pattern, string=gptstring, flags=re.MULTILINE|re.DOTALL)\n",
    "       if match:\n",
    "              sub_string = match[0]\n",
    "              return sub_string\n",
    "       \n",
    "gpt2sql(test_response['content'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "*Chefs kiss*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining\n",
    "As mentioned previously, each call has a character limitation. Since we know we get better output by prompting the model with the database schema and sample rows we will hit character limiations pretty quick. There is where chaining API calls comes to play. First i can ask the model which table i should query based on a short list of the tables and a brief description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flow.core.fact_transactions\n"
     ]
    }
   ],
   "source": [
    "input_prompt = 'How many transactions where on july 21st 2022?'\n",
    "\n",
    "# settings.tbl_descriptions taken from documentation\n",
    "# tbl_descriptions = {\n",
    "#     \"flow.core.ez_nft_sales\": 'this ez view is the same data table as the prior fact_nft_sales view. nft market sales on the flow blockchain. currently supported in this ez view are transactions that interact with the primary nft marketplace contract (), the fabricant nft marketplace contract, and nba topshot primary market sales. more markets can and will be added.'\n",
    "#     ,\"flow.core.ez_bridge_transactions\" : 'this ez_ view is the same data table as the prior fact_bridge_transactions view. this table parses transactions where tokens are bridged to or from the flow network using blocto teleport or the celer bridge.'\n",
    "#     ,\"flow.core.ez_staking_actions\" : 'this table provides transaction-level info on flow staking activities.'\n",
    "#     ,\"flow.core.ez_swaps\" : 'this table records asset swaps on the flow blockchain.'\n",
    "#     ,\"flow.core.ez_token_transfers\" : 'this table records all token transfers on the flow blockchain.'\n",
    "#     ,\"flow.core.fact_transactions\" : 'this table records all the transactions of the flow blockchain.'\n",
    "# }\n",
    "def ask_which_table(message):\n",
    "    system_prompt = \"You are a blockchain SQL master, you know everything about flow blockchain and can answer any enquiry.\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": json.dumps(settings.tbl_descriptions, indent = 4)  + \"\\n\\n\" + message},\n",
    "            {\"role\": \"assistant\", \"content\": \"I now have the table descriptions, i can advise which tables will be best to query\"},\n",
    "            {\"role\": \"user\", \"content\": \"Out of the tables mentioned, which table should i query? You answer should ONLY include the table name in the format 'core.flow.?' where the ? is the table name\\n\\nTable Name:\"},\n",
    "        ]\n",
    "    )\n",
    "    string = response.choices[0].message.content\n",
    "    table = re.findall(pattern=r\"flow\\.core\\.([\\w]+)\", string=string, flags=re.MULTILINE|re.DOTALL)\n",
    "    return \"flow.core.\" + table[0]\n",
    "\n",
    "table_name = ask_which_table(input_prompt)\n",
    "print(table_name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom. Lets keep the chain, chaining on. From the selected table throw her back in with the correct schema and sample data, and lets see what SQL we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_for_sql(message, table_name, check_sql=None):\n",
    "    system_content=f\"Given the SQL table:\\n\\n{settings.tbl_schema[table_name]}\\n\\nand sample data\\n\\n{settings.tbl_data[table_name]}\"\n",
    "    user_content = 'You are an SQL master. Only you can answer the following questions with strict snowflake sql code, nothing else. dont forget to name columns.'\n",
    "    assistant_content = \"I now have the db schema, i can write sql code to create a table\"\n",
    "    if check_sql is None:\n",
    "        question_content = f\"Write strict snowflake sql code to answer the following: {message}\"\n",
    "    else:\n",
    "        question_content = f\"Rewrite strict snowflake sql code to answer the following, question {message} \\n\\n {check_sql}\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content},\n",
    "            {\"role\": \"user\", \"content\": question_content},\n",
    "        ]\n",
    "    )\n",
    "    string = response.choices[0].message.content\n",
    "    if gpt2sql(string) is not None:\n",
    "        return gpt2sql(string)\n",
    "    else:\n",
    "        return string\n",
    "    \n",
    "sql_draft=ask_for_sql(input_prompt, table_name)\n",
    "sql=ask_for_sql(input_prompt, table_name, sql_draft)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "Added a little check_sql prompt so i could feed the sql back in and see if it comes with the same response. Literature told me this is a good idea and drastically reduces GPT making up table names, columns etc.\n",
    "\n",
    "Otherwise appears to be working well!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flipside\n",
    "Now we have working SQL code, we need to feed it back to flipside to query the actual data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'columns': ['num_transactions', '__row_index'], 'rows': [[671426, 0]]}\n"
     ]
    }
   ],
   "source": [
    "from flipside import Flipside \n",
    "\n",
    "def ask_flipside(sql):\n",
    "    # My first query\n",
    "    load_dotenv()\n",
    "    FLIPSIDE_API_KEY = os.getenv('FLIPSIDE_API_KEY')\n",
    "\n",
    "    # Initialize `Flipside` with your API Key and API Url\n",
    "    flipside = Flipside(FLIPSIDE_API_KEY, \"https://api-v2.flipsidecrypto.xyz\")\n",
    "\n",
    "    # Run the query against Flipside's query engine and await the results\n",
    "    query_result_set = flipside.query(sql)\n",
    "    # print('flipside query: ' + query_result_set.status)\n",
    "    return query_result_set\n",
    "\n",
    "result = ask_flipside(sql)\n",
    "table = {'columns': result.columns, 'rows': result.rows}\n",
    "print(table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function asking the model to comment on our output with a given question\n",
    "def ask_comments(message, table_result):\n",
    "    system_content=f\"Given the question:\\n\\n{message}\\n\\nand answer\\n\\n{table_result}\"\n",
    "    user_content = 'You blockchain business analyts.'\n",
    "    assistant_content = \"I now have the information i need to write comments\"\n",
    "    question_content = f\"Rewrite the answer table in words to explain the answer to the question\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content},\n",
    "            {\"role\": \"user\", \"content\": question_content},\n",
    "        ]\n",
    "    )\n",
    "    string = response.choices[0].message.content\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Asking GPT3 to decide which table...', 'GPT3 decided on table: flow.core.fact_transactions', 'Asking GPT3 to write SQL code...', \"GPT3 wrote SQL code: SELECT COUNT(*) as num_transactions\\nFROM flow.core.FACT_TRANSACTIONS\\nWHERE DATE_TRUNC('day', BLOCK_TIMESTAMP) = '2022-07-21';\", 'Asking GTP3 to check the SQL code...', \"GPT3 checked SQL code: SELECT COUNT(*) AS num_transactions\\nFROM flow.core.FACT_TRANSACTIONS\\nWHERE BLOCK_TIMESTAMP BETWEEN '2022-07-21 00:00:00' AND '2022-07-21 23:59:59';\", 'Asking Flipside to run the SQL code...', 'Flipside ran the SQL code and got this result: QUERY_STATE_SUCCESS', 'GPT3 wrote comments: '] /n The query result shows that there were 671,426 transactions that occurred on July 21st, 2022. The result was obtained within 7 seconds of executing the query. There was only one row of data returned by the query. No errors were encountered during the execution of the query. /n {'columns': ['num_transactions', '__row_index'], 'rows': [[671426, 0]]}\n"
     ]
    }
   ],
   "source": [
    "# The master chain\n",
    "def llm_chain(user_prompt):\n",
    "    logs = []\n",
    "    table =  {\n",
    "        \"columns\":'',\n",
    "        'rows':''\n",
    "        }\n",
    "    logs.append('Asking GPT3 to decide which table...')\n",
    "    try:\n",
    "        table_name = ask_which_table(user_prompt)\n",
    "        logs.append('GPT3 decided on table: ' + table_name )\n",
    "    except:\n",
    "        logs.append('GPT3 could not decide on table')\n",
    "        response = 'error'\n",
    "        return response, logs, table, \n",
    "    \n",
    "    logs.append('Asking GPT3 to write SQL code...')\n",
    "    try:\n",
    "        sql = ask_for_sql(user_prompt, table_name)\n",
    "        logs.append('GPT3 wrote SQL code: ' + sql )\n",
    "    except:\n",
    "        logs.append('GPT3 could not write SQL code')\n",
    "        response = 'error'\n",
    "        return response, logs, table\n",
    "    \n",
    "    logs.append('Asking GTP3 to check the SQL code...')\n",
    "    try:\n",
    "        sql = ask_for_sql(user_prompt, table_name, sql)\n",
    "        logs.append('GPT3 checked SQL code: ' + sql)\n",
    "    except:\n",
    "        logs.append('GPT3 could not check SQL code')\n",
    "        response = 'error'\n",
    "        return response, logs, table\n",
    "    \n",
    "    logs.append('Asking Flipside to run the SQL code...')\n",
    "    try:\n",
    "        result = ask_flipside(sql)\n",
    "        logs.append('Flipside ran the SQL code and got this result: ' + result.status )\n",
    "    except:\n",
    "        logs.append('Flipside could not run the SQL code')\n",
    "        response = 'error'\n",
    "        return response, logs, table\n",
    "    try:\n",
    "        response = ask_comments(user_prompt, result)\n",
    "        logs.append('GPT3 wrote comments: ')\n",
    "    except:\n",
    "        logs.append('GPT3 could not write comments')\n",
    "        response = 'error'\n",
    "        return response, logs, table\n",
    "    table =  {\n",
    "        \"columns\":result.columns,\n",
    "        'rows':result.rows\n",
    "        }\n",
    "    return response, logs, table       \n",
    "\n",
    "input_prompt = \"How many transactions where on july 21st 2022?\"\n",
    "response, logs, table = llm_chain(input_prompt)\n",
    "print(logs, '/n', response, '/n', table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Indeed, the FlowGPT AI chatbot shows promising potential. It has demonstrated its capability to handle complex queries and deliver precise responses based on specific data tables. However, to enhance its performance and extend its applicability, it does require additional testing and an expansion of training data.\n",
    "\n",
    "Incorporating a list of one-shot prompts can expedite responses for frequently asked questions or common queries. This could improve user experience and make the chatbot more efficient.\n",
    "\n",
    "Furthermore, while the current model doesn't explicitly handle join operations, its underlying architecture and training show that it's capable of formulating such operations. During the initial testing, the model was able to generate queries involving joins, showing its potential to handle more complex data requests.\n",
    "\n",
    "In conclusion, while the FlowGPT AI chatbot is already a powerful tool for querying Flow data, there is scope for further improvement and expansion. With continued development and refinement, it could become an even more valuable resource for data analytics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[Paper:Evaluating the Text-to-SQL Capabilities of Large Language Models](https://arxiv.org/abs/2204.00498)\n",
    "\n",
    "[Blog: Fine-Tuning GPT-3 for Natural Language to SQL Conversion: An In-Depth Guide](https://blog.futuresmart.ai/fine-tuning-gpt-3-for-natural-language-to-sql-conversion-an-in-depth-guide#heading-gpt-3-parameters)\n",
    "\n",
    "[Documentation: LangchainSQL](https://python.langchain.com/en/latest/modules/chains/examples/sqlite.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
